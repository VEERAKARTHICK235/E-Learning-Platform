import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    return x * (1 - x)

# XOR inputs and outputs
x = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])

# Random weights and biases
np.random.seed(1)
w1 = np.random.rand(2,2)
b1 = np.random.rand(1,2)
w2 = np.random.rand(2,1)
b2 = np.random.rand(1,1)

lr = 0.1  # learning rate

# Training
for i in range(10000):
    # Forward
    h_input = np.dot(x, w1) + b1
    h_output = sigmoid(h_input)
    final_input = np.dot(h_output, w2) + b2
    output = sigmoid(final_input)

    # Error
    error = y - output

    # Backward
    d_output = error * sigmoid_deriv(output)
    d_hidden = d_output.dot(w2.T) * sigmoid_deriv(h_output)

    # Update weights and bias
    w2 += h_output.T.dot(d_output) * lr
    b2 += np.sum(d_output, axis=0, keepdims=True) * lr
    w1 += x.T.dot(d_hidden) * lr
    b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr

# Final Output
print("Predicted Output:")
print(np.round(output, 3))

